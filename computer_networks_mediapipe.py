# -*- coding: utf-8 -*-
"""computer networks mediapipe

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1guG34-smpsU9734CjFRvv_0IqY-659sE

A Taste of the Future - Real-Time Vision with MediaPipe

### **Setup: Installing and Importing Libraries**

For this module, we'll need a few key libraries. We will install specific, known-good versions to ensure compatibility and prevent installation errors.
- **`mediapipe==0.10.14`**: The core Google library for our vision tasks. We pin this version for stability.
- **`opencv-contrib-python==4.9.0.80`**: An expanded version of OpenCV for video processing.
- **`numpy` & `matplotlib`**: Our standard tools for numerical operations and plotting.
"""

# Commented out IPython magic to ensure Python compatibility.
# Install a version of NumPy less than 2 before other libraries to avoid compatibility issues
# %pip install numpy"<2" mediapipe==0.10.14 opencv-contrib-python==4.9.0.80 matplotlib

# Commented out IPython magic to ensure Python compatibility.
import cv2
import mediapipe as mp
import numpy as np
import matplotlib.pyplot as plt
import os
import time
from google.colab import files
from IPython.display import clear_output

# %matplotlib inline

# Helper function to display images in a consistent way
def display_image(title, image, is_bgr=True):
    # Convert BGR to RGB for correct display in Matplotlib
    if is_bgr:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    else:
        image_rgb = image
    plt.figure(figsize=(10, 10))
    plt.imshow(image_rgb)
    plt.title(title, fontsize=16)
    plt.axis('off')
    plt.show()

# Check if running in Google Colab
IS_COLAB = 'google.colab' in str(get_ipython())

print("Libraries imported successfully!")
print(f"Running in Google Colab: {IS_COLAB}")

"""---

## **Part 1: Human Pose Estimation on a Static Image**

**Human Pose Estimation** is the task of identifying the key joints and landmarks of a person's body. MediaPipe's `Pose` model can detect 33 such keypoints.

Let's start by loading an image of an athlete to see how it works.
"""

image_path = '/content/assest /Screenshot 2025-11-24 195851.png'
try:
    image = cv2.imread(image_path)
    if image is None:
        raise FileNotFoundError
    print(f"Successfully loaded image from '{image_path}' with shape {image.shape}")
    display_image("Original Image", image)
except FileNotFoundError:
    print(f"Error: The image file was not found at '{image_path}'.")
    print("Please ensure the 'Tiger_Woods.png' file is in the 'assets' folder.")
    image = None # Set image to None if loading fails

"""### **1.1 Initializing the MediaPipe Pose Model**

First, we'll initialize the main `Pose` object. We'll also set up a `drawing_utils` object, which contains helper functions to easily visualize the results.
"""

# Initialize MediaPipe Pose and Drawing utilities
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

print("MediaPipe Pose utilities initialized.")

from google.colab import drive
drive.mount('/content/drive')

"""### **1.2 Processing the Image and Drawing Landmarks**

The core of the process is simple:
1.  Create an instance of the `Pose` model.
2.  Pass our image (converted to RGB) to its `process()` method.
3.  Use `drawing_utils` to draw the detected landmarks and connections onto a copy of our image.
"""

if image is not None:
    # Create a Pose instance. static_image_mode=True is for single images.
    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:
        print("Processing image to find pose landmarks...")
        # Convert the BGR image to RGB before processing
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = pose.process(image_rgb)

        # Make a copy of the original image to draw on
        annotated_image = image.copy()

        if results.pose_landmarks:
            print("Pose landmarks detected! Drawing landmarks and connections...")
            # Draw the pose annotation on the image.
            mp_drawing.draw_landmarks(
                annotated_image,
                results.pose_landmarks,
                mp_pose.POSE_CONNECTIONS, # This tells it to draw the lines connecting the points
                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=4),
                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)
            )
            print("Drawing complete.")
            display_image("Pose Landmarks and Connections", annotated_image)
        else:
            print("No pose landmarks were detected in the image.")
else:
    print("Skipping pose estimation because the image could not be loaded.")

"""### **1.3 Accessing Landmark Coordinates**

Visualizing is great, but for a real application, we need the actual coordinates of the landmarks. The `results` object contains all 33 landmarks, each with `x`, `y`, and `z` coordinates (normalized between 0.0 and 1.0). We can easily convert these to pixel coordinates.
"""

if image is not None and results.pose_landmarks:
    # Get the height and width of the image
    h, w, _ = image.shape

    # Get the landmarks for the left and right wrist
    left_wrist = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]
    right_wrist = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]

    # Calculate the pixel coordinates
    left_wrist_x = int(left_wrist.x * w)
    left_wrist_y = int(left_wrist.y * h)
    right_wrist_x = int(right_wrist.x * w)
    right_wrist_y = int(right_wrist.y * h)

    print(f"Left Wrist Pixel Coordinates: (x={left_wrist_x}, y={left_wrist_y})")
    print(f"Right Wrist Pixel Coordinates: (x={right_wrist_x}, y={right_wrist_y})")

    # We could now calculate things like the distance between the wrists
    distance = np.sqrt((left_wrist_x - right_wrist_x)**2 + (left_wrist_y - right_wrist_y)**2)
    print(f"\nPixel distance between wrists: {distance:.2f} pixels")
else:
    print("Skipping coordinate calculation because no landmarks were detected.")

"""---

---

<details>
<summary>Click here for the solution to Challenge B</summary>

```python
    # --- SOLUTION --- #
    if left_wrist_y < left_shoulder_y and right_wrist_y < right_shoulder_y:
        gesture_text = "Hands Up!"
        gesture_color = (0, 255, 0) # Green
    # ---------------- #
```
</details>
"""